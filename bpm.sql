-- MySQL dump 10.13  Distrib 5.6.17, for Win64 (x86_64)
--
-- Host: localhost    Database: pattern_table
-- ------------------------------------------------------
-- Server version	5.6.22-log

/*!40101 SET @OLD_CHARACTER_SET_CLIENT=@@CHARACTER_SET_CLIENT */;
/*!40101 SET @OLD_CHARACTER_SET_RESULTS=@@CHARACTER_SET_RESULTS */;
/*!40101 SET @OLD_COLLATION_CONNECTION=@@COLLATION_CONNECTION */;
/*!40101 SET NAMES utf8 */;
/*!40103 SET @OLD_TIME_ZONE=@@TIME_ZONE */;
/*!40103 SET TIME_ZONE='+00:00' */;
/*!40014 SET @OLD_UNIQUE_CHECKS=@@UNIQUE_CHECKS, UNIQUE_CHECKS=0 */;
/*!40014 SET @OLD_FOREIGN_KEY_CHECKS=@@FOREIGN_KEY_CHECKS, FOREIGN_KEY_CHECKS=0 */;
/*!40101 SET @OLD_SQL_MODE=@@SQL_MODE, SQL_MODE='NO_AUTO_VALUE_ON_ZERO' */;
/*!40111 SET @OLD_SQL_NOTES=@@SQL_NOTES, SQL_NOTES=0 */;

--
-- Table structure for table `about`
--

DROP TABLE IF EXISTS `about`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `about` (
  `ID` int(11) NOT NULL AUTO_INCREMENT,
  `Description` longtext,
  PRIMARY KEY (`ID`),
  UNIQUE KEY `ID_UNIQUE` (`ID`)
) ENGINE=InnoDB AUTO_INCREMENT=8 DEFAULT CHARSET=utf8;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `about`
--

LOCK TABLES `about` WRITE;
/*!40000 ALTER TABLE `about` DISABLE KEYS */;
INSERT INTO `about` VALUES (1,'The Event Log Imperfection Patterns initiative started as a result of work being done by Arthur ter Hofstede, Suriadi Suriadi, Moe Wynn and Robert Andrews in 2015 on process mining in the healthc are domain.<br><br>During the data preparation stage of the process mining analysis, it became apparent that characteristics of the structure and (user) behaviour of the information systems used to generate the source event logs for the analysis resulted in the same types of data quality issues arising over and over again. These quality issues could be recognised by their \'signature\', that is, the way they manifested in the logs. Further, they could be described as patterns to make automated identification of these quality issues possible. <br><br>Once a quality issue has been defined as a pattern, it is also possible to structure remedial action as a pattern, facilitating automated rectification of the quality issue.<br><br>The Event Log Imperfection Patterns set is a collection of typical problems that may be encountered in event logs as well as associated remedies that can be used to rectify them and forms a repository of knowledge to deal with data imperfections.<br><br>The patterns remain agnostic of any subsequent form of analysis (process discovery or process conformance) and can thus be applied in the first stages of a process mining analysis.<br><br>It is entirely likely, that as further data sets are analysed, the set of Event Log Imperfection patterns will grow.<br><br>The use of the patterns in rectifying quality issues in event logs is a small step towards a systematised approach to conducitn a process mining analysis.');
/*!40000 ALTER TABLE `about` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `contact_information`
--

DROP TABLE IF EXISTS `contact_information`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `contact_information` (
  `Id` int(11) NOT NULL AUTO_INCREMENT,
  `Name` longtext,
  `Email` longtext,
  `Affiliation` longtext,
  `Role` longtext,
  `PurposeOfContact` longtext,
  `ResearchExperience(year)` varchar(45) DEFAULT NULL,
  `ResearchTools` longtext,
  `DatasetAnalysed` int(11) DEFAULT NULL,
  `FamiliarityWithProcessMining` longtext,
  PRIMARY KEY (`Id`)
) ENGINE=InnoDB AUTO_INCREMENT=90 DEFAULT CHARSET=utf8;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `contact_information`
--

LOCK TABLES `contact_information` WRITE;
/*!40000 ALTER TABLE `contact_information` DISABLE KEYS */;
/*!40000 ALTER TABLE `contact_information` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `document`
--

DROP TABLE IF EXISTS `document`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `document` (
  `Id` int(11) NOT NULL AUTO_INCREMENT,
  `Section` varchar(45) DEFAULT NULL,
  `Sub_section` varchar(90) DEFAULT NULL,
  `Author` varchar(45) DEFAULT NULL,
  `Title` varchar(45) DEFAULT NULL,
  `URL` varchar(45) DEFAULT NULL,
  `Publisher` varchar(45) DEFAULT NULL,
  PRIMARY KEY (`Id`),
  UNIQUE KEY `Id_UNIQUE` (`Id`)
) ENGINE=InnoDB AUTO_INCREMENT=7 DEFAULT CHARSET=utf8;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `document`
--

LOCK TABLES `document` WRITE;
/*!40000 ALTER TABLE `document` DISABLE KEYS */;
INSERT INTO `document` VALUES (1,'Key Papers','<div id=\"Event Log Imperfection Patterns\">Event Log Imperfection Patterns</div>','Malcolm Turnbull','Turnbull\'s report','documents/BPM-06-22.pdf','Liberal Publication'),(2,'Other patterns writings','','Tony Abbott','Abbott\'s report','documents/data_patterns BETA TR.pdf','Liberal Publication'),(3,'Key Papers','<div id=\"Data Quality\">Data Quality</div>','Kevin Rudd','Rudd\'s report - Data Patterns','documents/wfs-pat-2002.pdf','Labor Publication'),(4,'Evaluations',NULL,'Julia Gillard','Gilard\'s report','documents/data_patternsER2005.pdf','Labor Publication'),(5,'Key Papers','<div id=\"Event Log Quality\">Event Log Quality</div>','Kevin Rudd','Rudd\'s report - Control-Flow Patterns','documents/Resource Patterns BETA TR.pdf','Labor Publication'),(6,'Evaluations',NULL,'John Howard','Howard\' report',NULL,'Liberal Publication');
/*!40000 ALTER TABLE `document` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `impact`
--

DROP TABLE IF EXISTS `impact`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `impact` (
  `ID` int(11) NOT NULL AUTO_INCREMENT,
  `Description` longtext,
  PRIMARY KEY (`ID`),
  UNIQUE KEY `ID_UNIQUE` (`ID`)
) ENGINE=InnoDB AUTO_INCREMENT=4 DEFAULT CHARSET=utf8;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `impact`
--

LOCK TABLES `impact` WRITE;
/*!40000 ALTER TABLE `impact` DISABLE KEYS */;
INSERT INTO `impact` VALUES (1,'It is hoped that, over time, the Event Log Imperfection patterns will be used more frequently as a means of describing quality issues in event logs. This page provides links to lists of references that apply or discuss the event log imperfection patterns in various settings.<br><br>Citations that apply to:'),(2,'<a href =\"Documentation.php#Event Log Quality\">Event Log Quality</a><br><a href =\"Documentation.php#Data Quality\">Data Quality</a><br><a href =\"Documentation.php#Event Log Imperfection Patterns\">Event Log Imperfection Patterns</a>');
/*!40000 ALTER TABLE `impact` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `link`
--

DROP TABLE IF EXISTS `link`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `link` (
  `LinkId` int(11) NOT NULL AUTO_INCREMENT,
  `LinkText` longtext,
  `URL` varchar(60) DEFAULT NULL,
  `Description` longtext,
  PRIMARY KEY (`LinkId`),
  UNIQUE KEY `LinkId_UNIQUE` (`LinkId`)
) ENGINE=InnoDB AUTO_INCREMENT=4 DEFAULT CHARSET=utf8;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `link`
--

LOCK TABLES `link` WRITE;
/*!40000 ALTER TABLE `link` DISABLE KEYS */;
INSERT INTO `link` VALUES (1,'Workflow Patterns','http://www.workflowpatterns.com/links/',' A joint effort of Eindhoven University of Technology (led by Professor Wil van der Aalst) and Queensland University of Technology (led by Professor Arthur ter Hofstede) which started in 1999.'),(2,'QUT','http://www.qut.edu.au/','QUT\'s homepage'),(3,'TU/e','https://www.tue.nl/en/','TU/e\'s homepage');
/*!40000 ALTER TABLE `link` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `patterns`
--

DROP TABLE IF EXISTS `patterns`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `patterns` (
  `Id` int(11) NOT NULL AUTO_INCREMENT,
  `Description` longtext,
  `RealLifeExample` longtext,
  `Affect` longtext,
  `Manifestation` longtext,
  `Category` varchar(20) DEFAULT NULL,
  `Remedy` longtext,
  `SideEffect` longtext,
  `QualityIssueName` longtext,
  `QualityIssue` longtext,
  `PatternName` longtext NOT NULL,
  PRIMARY KEY (`Id`),
  UNIQUE KEY `id_UNIQUE` (`Id`)
) ENGINE=InnoDB AUTO_INCREMENT=13 DEFAULT CHARSET=utf8;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `patterns`
--

LOCK TABLES `patterns` WRITE;
/*!40000 ALTER TABLE `patterns` DISABLE KEYS */;
INSERT INTO `patterns` VALUES (1,'This pattern may occur where the data in an event log is captured from electronic-based forms (e.g. a patient test results form). Users (such as nurses and doctors) click a \"Save\" button to trigger the recording of the data captured by the form, often with the undesirable side effect of associating all data captured by the form with the same timestamp (the time the user clicked the \"Save\" button). The more interesting information about the ordering of activities from which the values of the data inserted into the forms were derived, such as the time a blood sample was taken, is flattened into one timestamp. This method of data recording can result in an additional problem. When the form is updated at some point in the future, the system may actually store all data captured by the form again, even though only a few data items were actually changed. As a result, the log data stored by such a system contains redundant information due to the fact that some data items in the form were not changed but were re-recorded in the storage with a new timestamp (the time when the update event happened).','','This imperfection pattern \"flattens\" the temporal ordering of events as they are all assumed to have happened at the same time resulting in the actual ordering of events being lost. Ignoring this data pattern may result in the discovery of complex process models as all events sharing the same timestamp in a case will likely be treated as parallel events, and consequently, be modeled in a parallel manner. Modeling parallel events graphically tends to increase the number of arcs (and the often unavoidable cross-cuttings of arcs) in the models, thus making them difficult to comprehend. Finally, this imperfection pattern may produce unnecessary duplication of events as certain events may be re-recorded as a result of the updates of a few other data items within the same form. This situation is likely to result in the extraction of misleading process mining analysis results due to the existence of events in the log that did not actually happen.',' This pattern\'s signature is the existence of groups of events in a log with the same case identifier and timestamp value. The signature of this pattern can be detected manually by looking at the event log for groups of events with the same case identifier and timestamp value. Alternatively, the log can be searched for the presence of \"marker\" events with activity names similar to field labels known to exist on the same form (assuming that such information about the forms can be obtained from system users). If found, the timestamps of the \"marker\" events can be checked to see if they are the same. Regardless, the regular occurrences of groups of events that share the same timestamp value is already a good indication of the presence of the \"Form-based Event Capture\" pattern.','Event','The simplest remedial action is to aggregate all events, within each group of events having the same timestamp, into one event only. An additional attribute can be created for this event with a complex data structure to capture relevant data from the events that have been \"collapsed\" into the aggregated event. This approach removes all other events that have been recorded from one form, thus reducing the amount of parallelism in the discovered models. However, such a remedy can only be applied if it is sensible to represent the information collected from the form as one process step. For example, if all events with the same timestamp reflect nothing more than a nurse performing various types of medical checks on a patient where all of those checks fall under the umbrella of \"vital signs checks\" activity, then we can aggregate them into one event named \"Vital Sign Checks\". However, if those events with similar timestamps contain two or more distinct and/or important process steps that need to be explicitly considered in the analysis, important information may be lost through simply performing events aggregation. Instead, each group of events will need to be aggregated into at least two or more events reflecting the distinct steps taken. These aggregated events, will however, still share the same timestamp  (some of which may be duplicate events because their values did not require any updates). Properly addressing this scenario requires firstly an understanding of how updates are recorded in the event log. There are two situations: (i) the updates of one or more data items in a form result in the recording of all fields as events in the log, or (ii) the updates of one or more fields in a form will result in the recording of only those fields whose values have changed. In the former case, it is necessary to identify the specific information that has changed. In both cases, it is important to note what process step(s) may have taken place in order for the data items in the forms to be updated, and then aggregate those changed data items into one or more events as needed.','Where a process requires a form update, it may be the case that, for a given activity, the \"new\" value of a data item is the same as the \"old\" value of the data item. Where the form logging mechanism writes out all data values in the form, the fact that the \"old\" and \"new\" values of the data item are the same makes it difficult to determine whether the activity was carried out a second time or the data item was simply re-written as part of the form update process. In this scenario, we may lose the \"update\" action on those fields where \"new\" values and \"old\" values are the same.',' I16 - Incorrect data: timestamp, I27 - Irrelevant data: event','The temporal flattening introduced into an event log through the occurrence of this pattern negatively impacts the attribute accuracy of the log in that the timestamps of the events reflect the saving of the form rather than the performance of the event. Further, if the system records all fields on the form rather than only those fields that have changed, the trace completeness may be affected through erroneous inclusion of events that did not actually happen in the case.','Form-based Event Capture'),(2,'This pattern captures the situation where certain entries in a log are recorded with an erroneous timestamp due to the \"proximity\" of the correct data value and the incorrect data value. Here \"proximity\" refers to a situation where two timestamp values are so close to each other that human error can inadvertently result in incorrect timestamp values to be recorded. A typical example of a proximity error pattern is the recording of incorrect timestamps for events that happen just after midnight. Often, the date portion of the timestamp is incorrectly recorded as the date of the previous day, while the time portion of the timestamp is recorded correctly. Another example of this pattern is the recording of incorrect timestamps of events due to users simply pressing the wrong key(s) on a keyboard i.e. inadvertently pressing keys adjacent to the intended key(s).',NULL,' The existence of this pattern will result in incorrect models being discovered as the models are likely to allow behaviours that did not occur in reality. This pattern will also impact the accuracy of time related performance analysis results (e.g. working time and waiting time), although the impact may not be serious if such patterns occur in only a relatively small number of cases.','The pattern typically manifests itself by the existence of a number of cases in which event ordering deviates significantly. That is, there exist arcs in the discovered process models that allow one or more activities to be directly reached from activities which logically could not have happened. The existence of such arcs can also be discovered by examining the details of each process variant in the log. To detect this pattern, it is not sufficient that arcs which allow unusual behaviours are present in the discovered model. Once such arcs are discovered, it is necessary to go back to the log and test if simply changing one of the misplaced events\' imestamp components (e.g. the day component) is sufficient to \"place\" those events back to their proper ordering. At this point, it is quite reasonable to say that such a pattern exists in the log. For example, consider a process with only three sequential activities A, B, and C. If the log contains a trace with the following event ordering: A (2011-09-25 21:56:23), B (2011-09-25 00:23:11), and C (2011-09-26 01:34:56), the discovered process model will have an arc from B to A, and from A to C. However, in the domain, the \"ground truth\" is that B can only occur after A and before C. If the date component of the timestamp of B is modified from \"25\" to \"26\", activity B will be in the proper order. In this scenario, the \"Inadvertent Time Travel\" pattern exists in the event log.','Event',' Addressing this pattern in a generic manner requires knowledge of the minimum restrictions applicable to the ordering of all activities in the log. With this knowledge, each trace in the log can be examined to identify the existence of traces that violate the minimum ordering restrictions. Once discovered, the timestamp of events in the traces that violate the ordering restriction can be \"fixed\" by applying various remediations for standard proximity errors (e.g. adding or subtracting one day from the timestamp, or flipping the value of the timestamps based on proximity of the keys in a  standard keyboard layout). If one of those fixes re-orders the events in the trace such that the trace no longer violates the ordering restrictions, the erroneous old timestamp value should be replaced with the new value. Otherwise, it may be that the original timestamp value does not suffer from the \"Inadvertent Time Travel\" pattern after all (though the value may still be incorrect).',NULL,'I16 - Incorrect data: timestamp',' The incorrect timestamp values introduced into an event log through the occurrence of this pattern negatively impacts the attribute accuracy of the log in that the temporal ordering of the events no longer reflects the actual ordering of events.','Inadvertent Time Travel'),(3,'This pattern refers to a situation where the timestamp values of an event log are recorded in a format that is different from that which is expected by the tools used to process the event log. Consequently, the loading of the event log into those tools will result in incorrect interpretation of timestamp values. Typical format variations include the confusion between month-day vs. day-month format, the use of colon (\":\") symbol vs. dot (\".\") symbol to separate between hour, minute, and second information, and the varying manner in which timezone information is encoded. This pattern is likely to occur when the event log is constructed from multiple sources.',NULL,'Incorrect timestamp values will adversely affect process mining results.It is not difficult to see how a process model discovery exercise for example, may output process models that are substantially different from reality in both event ordering and case duration.','This pattern generally manifests itself when processing an event log. Some tools produce error messages when used to load an event log with an incompatible timestamp format. However, tools, such as Microsoft Excel, are quite \"\"relaxed\"\" in the way they parse timestamp information (as they often have built-in intelligence to detect the correct timestamp format) and may not produce any\r warning or error messages, although the data may have been loaded incorrectly. If the latter, the detection of this data imperfection is more difficult. This pattern may also be detected through the discovery of process models with unexpected, and often incorrect, ordering of activities and the extraction of unreasonably long or short working and/or waiting times. Another indicator is the existence of missing timestamp information across many events in the log - due to the tool not being able to parse the timestamp information correctly and ignoring the values altogether.','Event',' To address this problem, it is necessary to ensure the tools used to import the event log do not inadvertently mis-interpret the timestamp information without producing any warnings. While tedious, the simplest way to handle this situation is to prevent the tools from interpreting certain fields in the log as \"timestamp\" information in the first place. This can be achieved by adding a few characters, such as asterisks, before and after the timestamp values to force the \"switching off\" of the built-in timestamp interpretation mechanism that many tools have. Following this, the file can be edited (with a text editor) and appropriate \"string\" manipulation techniques (such as find and replace) applied to reformat the string values as timestamps.',' In practice, there could indeed be events that were executed in a sequence that did not meet the \"expected\" ordering restrictions. Hence, this remedy may result in the loss of interesting deviant behaviours in the log.','I23 - Imprecise data: timestamp, I16 - Incorrect data: timestamp','The incorrect timestamp values introduced into an event log through the occurrence of this pattern negatively impacts the attribute accuracy of the log in that the temporal ordering of the events no longer reflects the actual ordering of events.','Unanchored Event'),(4,'This pattern refers to events in an event log which have attributes that contain further information that can be used to derive new events. In other words, there is information contained within an\nexisting event log that can be exploited to construct additional events, but, the information is hidden in attribute values of several events.','',' The existence of this pattern is not likely to add additional overheads or hinder the process mining exercise. However, it represents untapped information that could enrich the insights obtained from a process mining exercise.','This pattern manifests itself through event attribute/s (other than the timestamp attribute) where part of the attribute value could be interpreted as a timestamp and (possibly) part as additional information. To detect this pattern, one could search the values of all attributes in an event log (except the timestamp attribute) for possible timestamp-like information. This search could be automated. The existence of such timestamp-like information in one (or more) attributes of an event log (other than the \"designated\" timestamp attribute) may indicate that this pattern exists; however, it is not definitive as one still needs to look for further information within the same attribute or across other attributes of that event to determine if there is sufficient information to reconstruct a new event. Generally, the detection of this imperfection pattern requires manual effort and the assistance of domain experts as this \"hidden\" information (from which new events can be derived) could be encoded in practically any attribute value of an event log.','Event','Given the variety of manners in which this pattern may manifest itself, a generic solution to fix this issue is unlikely. Nevertheless, once the location of the information from which new events can be re-constructed is known, it is possible to develop a tool to automate the creation of the new events.','test',' I16 - Missing data: event','The occurrence of this pattern affects the trace completeness through omission of events that actually happened in the case but, due to the logging mechanisms in the underlying information system, were not recognised as such.','Scattered Event'),(5,'This pattern refers to a log in which events are not explicitly linked to their respective case identifiers. This pattern is often seen in an event log that is derived from a system that is not process-aware, or is not meant to support activities in the context of a process (e.g. a GPS tracking system, a web traffic log, or industrial devices). Consequently, the concept of a \"case\" cannot be trivially defined by simply using the information in the log as-is. Nevertheless, the notion of a \"case\" exists and can be discerned, especially by domain experts (e.g. a user web-browsing session or a journey from a geographical location A to another location B).','','Process mining requires each event be attributable to a case. The existence of this imperfection pattern will prevent a process mining analysis being conducted until the concept of what comprises a case has been resolved.','This pattern manifests itself in an event log where events cannot be linked to one another in any meaningful way, due to the lack of information necessary to group events into cases. The\r \"Elusive Case\" pattern can be said to occur where there does not exist: (i) a set of attributes (one or more), (ii) common to all events, (iii) for which the value of the attribute/s in each event can be used to group events into a case (i.e. all events relate to the same process instance and collectively capture all activities of a case that one can reasonably expect to occur).\r \r This pattern can also be detected by randomly tagging one or more attributes as the \"\"case identifier\"\" attribute(s) of an event log and then attempting to discover a process model. One could do so when loading an event log into a process mining tool such as Disco. If the resulting model captures a complete process model with reasonable temporal dependencies between events, the tagged attribute can be reasonably used as the \"\"case identifier\"\" attribute for the log. In this situation, the \"\"Elusive Case\"\" does not exist in the log. If there is/are no attribute/s that, when tagged as case identifier, can deliver an acceptable process model, it is likely that the \"\"Elusive Case\"\" pattern exists in the log.','Case','To address this situation, the cases to which events belong need to be correctly identified. In most instances, this can be done by correlating information in the event log with information from another source.','Side effects of such a remedy include (i) incorrect mapping of events to their cases, and (ii) the omission of many events in the log as they cannot be mapped to any particular cases resulting in the loss of potentially important information.',' I16 - Missing data: relationships','The relationship between events and cases is missing.','Elusive Case'),(6,'This pattern describes a situation where key process steps are missing in the event log being analysed (thus not giving the complete picture of the activities involved in a case) but are recorded elsewhere (e.g. in a different system). This pattern then is concerned with constructing a complete picture of the cases in a log by merging information from different sources. The key challenge here is ascribing information extracted from different sources to the correct event log case identifier when each source may have its own unique identifier (the so called \"record linkage\" problem).','',' If not addressed properly, this event log imperfection pattern will result in discovered process models representing only a fraction of the total process due to the event log containing incomplete trace information.','The manifestation of this pattern is as \"gaps\" with regard to activities recorded in the log. For example, for all cases in a log, the time when a product was ordered is recorded, but, expected successor activities such as the picking the order and the dispatch of the order are not recorded. In this situation, it is clear that there is a segment of information that is missing from the\r event log which will need to be \"\"patched\"\" by drawing the required information from other sources. Where all the activities involved in the process being analysed are known, detection of this pattern involves comparing the list of unique activity names recorded in the log against expected activity names. A second check would be to match, for each activity, the respective frequencies of the activity, expected precursor activities and expected successor activities. A discrepancy could indicate at least partial trace incompleteness.','Case','This imperfection pattern can be redressed through application of an appropriate record linkage technique so that events from various sources can be merged into one log with events from the various sources being properly attributed to cases. The approach to merging will depend on the availability (or not) of some unique identifier. Where all data sources use exactly the same case identifiers, the source logs can simply be merged into one event log. Where individual source logs use different case identifiers, but there is a global unique identifier that can be used to establish the link between the different case identifiers, e.g. through the use of some \"Master Record Number\", the global unique identifier should be used as the merge key with the case identifier being set as either the global unique identifier or one of the existing case identifiers.','Where no global unique identifier can be determined, i.e. a record linkage algorithm has been applied, it is possible that events will not be properly attributed to cases due to false-positives and\r false-negatives generated by the linkage algorithm.',' I112 - Incorrect data: relationship','The association between events and cases are logged incorrectly from the domain perspective. That is, within each contributing system\'s log, events are correctly ascribed to cases, but as there is no common case identifier at the domain level, when the events from the contributing system\'s logs are combined to form a consolidated process level log, it is not possible to properly merge events into cases.','Scattered Case'),(7,'This pattern captures the situation where, within an event log, there are multiple events which essentially refer to one particular process step within a case. This could result from (i) the event log having been constructed using data from multiple systems, each of which with their own way of recording the same process activity, (ii) the underlying system used is programmed to automatically fire a set of auxiliary events when a specific event (such as the payment of a bill to a supplier) occurs, and/or (iii) the audit trail-like nature of the log which records detailed low-level user activities (such as the opening and closing of a form) such that it is common to see duplication of events within a very short time period (e.g. when a user is switching back and forth between two forms). Regardless, as these events exist independently from each other, their labels are likely to be different and their timestamps may also differ slightly.','',' From a process mining perspective, this pattern tends to create unnecessary noise in the data. These \"collateral\" events often refer to trivial low-level activities which do not contribute much to the extraction of meaningful insights about processes. Consequently, it is not uncommon for this pattern to result in highly-complex process mining results (e.g. overly complex models) that hinder the extraction of meaningful conclusions.','This pattern’s signature is the log containing groups of activities with timestamps that are very close to each other (e.g. within seconds). Further, the labels of these activities may be very similar to each other or are of logical consequences from one another, and, from the perspective of a domain expert, these events do not represent a distinct process step. For example, the submission of an insurance claim via an online claim submission system may fire a set of notifications and emails to both the customer and the claim handler(s), resulting in the occurrence of multiple events within a very short period of time. The detection of this pattern requires the knowledge of domain experts who are familiar with both the functionality of the systems (from which the event log is extracted) and the overall steps of the process being analysed. However, from our experience, the existence of multiple events occurring within a short period of time is already an indication of the existence of this pattern. Furthermore, the existence of a very high number of events in most of the traces in an event log (e.g. more than 100 events) is also a good indication of the existence of this pattern.','Case','The remedy for this pattern involves the development of a knowledge base that records information about those activity names that, when occurring together within a short time period, should be merged into one single activity. The name of the merged activity should be specified in the knowledge base too, including the timestamp that should be used (e.g. take the earliest or the latest timestamp). Essentially, this remedy will reduce the total number of unique activities in an event log. While this remedy seems similar to the remedy for the ‘Synonomous Labels’ pattern (discussed later) the type of knowledge-base required is quite different. The knowledge base for this pattern requires the knowledge of not just the domain expert, but also the knowledge of process analysts/system analysts who understand how process steps are created by the system used and subsequently stored as events in the logs, including the set of ‘collateral’ events that are triggered by the system. By contrast, the knowledge required to address the ‘Synonomous Labels’ pattern generally requires only the knowledge of a domain expert who understands the variations in the naming of activities.','','I27 - Irrelevant data: event','The inclusion of multiple low-level activities results in the masking of the actual business process step through the inclusion of multiple events in the log.','Collateral Events'),(8,'This pattern refers to the presence of a group of event attribute values that are structurally the same, yet are distinct from each other due to differences in the exact attribute values that further qualify the meaning of the value.',NULL,' Where the pattern exists and affects the attribute that serves as the activity name, the process mining analysis will result in the discovered process models over-fitting the event log as there will be too many specific activities that should have firstly been abstracted out. In general, if this pattern exists and affects attributes such as case identifiers, activity names, and resource identifiers that are critical for process mining analyses, the quality of the results will be negatively impacted as the set of values will have cardinality greater than the number of allowed values in real-life.','The signature of this pattern is the attribute value being composed of a mixture of immutable boiler-plate text and mutable text that occurs at predictable points among the immutable text. So, if there are two known fixed words (word 1 , word 2 ), the ‘Polluted Label’ pattern can be expressed with the following regular expression: [.]*?(word 1 )[.]*?(word 2 )[.]*. The existence of this pattern can be detected by either (i) checking the number of distinct values of each attribute (an unexpectedly high number of distinct values is a good indication of the existence of this pattern) followed by inspection of the values themselves to identify if there are those mutable and immutable parts within the values of that particular attribute, or (ii) using a (semi-)automated tool to cluster the values of that particular attribute, and within each cluster, to extract a regular pattern in a form that is similar to the one described above.','Log','The detection of the ‘Polluted Label’ pattern implies that the immutable word(s) of the pattern are known or can be determined. The mutable words can be removed and the immutable words rearranged to form one standard activity name. (Mutable words can be preserved as attribute values as required.)','','I15 - Incorrect data: activity name, I17 - Incorrect data: resource',' The existence of this pattern in the log, particularly where it affects the activity name, effectively masks the underlying process step through the incorrect logging of the activity name.','Polluted Label'),(9,'This pattern refers to the presence of a group of event attribute values that are structurally the same, yet are distinct from each other due to differences in the exact attribute values that further qualify the meaning of the value.',NULL,' Where the pattern exists and affects the attribute that serves as the activity name, the process mining analysis will result in the discovered process models over-fitting the event log as there will be too many specific activities that should have firstly been abstracted out. In general, if this pattern exists and affects attributes such as case identifiers, activity names, and resource identifiers that are critical for process mining analyses, the quality of the results will be negatively impacted as the set of values will have cardinality greater than the number of allowed values in real-life.','The signature of this pattern is the attribute value being composed of a mixture of immutable boiler-plate text and mutable text that occurs at predictable points among the immutable text. So, if there are two known fixed words (word 1 , word 2 ), the ‘Polluted Label’ pattern can be expressed with the following regular expression: [.]*?(word 1 )[.]*?(word 2 )[.]*. The existence of this pattern can be detected by either (i) checking the number of distinct values of each attribute (an unexpectedly high number of distinct values is a good indication of the existence of this pattern) followed by inspection of the values themselves to identify if there are those mutable and immutable parts within the values of that particular attribute, or (ii) using a (semi-)automated tool to cluster the values of that particular attribute, and within each cluster, to extract a regular pattern in a form that is similar to the one described above.','Log','The detection of the ‘Polluted Label’ pattern implies that the immutable word(s) of the pattern are known or can be determined. The mutable words can be removed and the immutable words rearranged to form one standard activity name. (Mutable words can be preserved as attribute values as required.)',NULL,'I15 - Incorrect data: activity name, I17 - Incorrect data: resource',' The existence of this pattern in the log, particularly where it affects the activity name, effectively masks the underlying process step through the incorrect logging of the activity name.','Distorted Label'),(10,' The ‘Synonymous Label’ pattern refers to a situation where there is a group of values (of certain attributes in an event log) that are syntactically different but semantically similar. This pattern is commonly encountered where an event log has been constructed by merging data from sources that do not share a common schema thus allowing the same real-world activity to be recorded with different labels in each source.',NULL,' Where this pattern affects the activity name, the readability and validity of process mining results are negatively impacted due to the inclusion of ‘behaviours’ in the discovered process models that should have been ‘merged’ (as they involve activities that share the same semantics). This pattern may also impact the performance analysis results through having two or more activities in the log that should be treated as the same activity, actually being considered as separate.','This pattern’s signature is the existence of multiple values of a particular attribute that seem to share a similar meaning but are nevertheless, distinct. For example, a particular resource is identified as ‘jsmith’ in some events and ‘Jason Smith’ in others. These two distinct values, in reality, refer to the same resource. Detection of this pattern may require the establishment of a knowledge base that stores the list of ‘acceptable’ values for each attribute in the log. Such a knowledge base will allow checking of the values of each attribute against the corresponding list of ‘acceptable’ values. The ‘Synonymous Label’ pattern exists when two or more attribute values correspond to the same value in the ‘acceptable’ list.','Log','Where syntactic differences between labels are minor, a text similarity search can be applied to group those events that have strong similarity in their labels, and then replace them with a predefined value. Where the syntactic differences are quite substantial (e.g. ‘CartLoaded’ vs. ‘OrderRecorded’), the use of an ontology will allow replacement of the labels with just one value (either one of the synonyms can theoretically be used as the label substitute).',' A label could be incorrectly mapped to another label such that the meaning of the original label somewhat deviates from the original meaning (or intent) of the label. This is likely to happen when the ontology used is flawed or when two labels share strong syntactic similarities but differ semantically, e.g. ‘drawn vs. dawn’.','I22 - Imprecise data: event attributes','The existence of multiple names for the same attribute creates ambiguity in an event log.','Synonymous Labels'),(11,'This pattern describes a situation where an activity is repeated multiple times within a case (same activity label applied to each occurrence of the activity), but the interpretation of the activity, from a process perspective, differs across the various occurrences. This pattern is likely to occur when an audit trail log is used to construct an event log. An audit trail log typically contains low-level records of ‘things that have been executed’ by users. The influence of contextual factors (such as the number of times an activity has been repeated within the same case) on the meaning of the activity being recorded is not captured.',NULL,'The existence of this pattern will result in discovered process models painting an incomplete picture of the process being analysed. Incomplete because the repeated activities, which actually have different meanings, are ‘grouped’ into one, thus ‘hiding’ certain process information. This is because most, if not all, process discovery algorithms do not treat repeated activities as separate activities in the discovered model.',' The signature of this pattern is the existence of an activity within a discovered process model that has many incoming arcs, often including a self-loop arc and arcs from other activities.\r \r  The presence of these arcs alone is an indication (but not confirmation) of the existence of the pattern in the log. A second indicator is the ratio of the number of times a particular activity occurs in a log and the total number of cases in the log. A high ratio indicates that the activity is repeated many times within a case and requires investigation to check if different interpretations are possible for each repetition of the activity. Domain knowledge (as to different meanings being ascribed to repeated occurrences of the activity ) is required to conclusively detect the existence of this pattern.','Log','This pattern can be addressed by explicitly relabeling repeated activities with a context sensitive name. Doing so requires firstly identifying the different contexts under which an activity can be repeated, developing a formula to assign the appropriate context to those homonymous activities, and then differentiating between them by adding context information into the label of those activities.','The decoupling of activity labels proposed in this remedy may result in too many distinct activity names, thus reducing the readability of the discovered process model and making the process\r mining analyses more complex and laborious.',' I2 - Imprecise data: activity name','The activity names are too coarse to reflect the different connotations associated with the recorded events.','Homonymous Labels'),(12,'<img id=\"animatedImage\"  src =\"images/s-sample.gif\"  data-still= \"images/s-sample.gif\"  data-animated=\"images/sample.gif\"  style =\"height:200px; width:200px;\"><br><h4>Hover to animate</h4>This pattern is made to demonstrate how images work<br>','No example found','','','Log','','','',NULL,'Demo (images)');
/*!40000 ALTER TABLE `patterns` ENABLE KEYS */;
UNLOCK TABLES;

--
-- Table structure for table `top`
--

DROP TABLE IF EXISTS `top`;
/*!40101 SET @saved_cs_client     = @@character_set_client */;
/*!40101 SET character_set_client = utf8 */;
CREATE TABLE `top` (
  `Id` int(11) NOT NULL AUTO_INCREMENT,
  `Description` longtext,
  PRIMARY KEY (`Id`),
  UNIQUE KEY `Id_UNIQUE` (`Id`)
) ENGINE=InnoDB AUTO_INCREMENT=2 DEFAULT CHARSET=utf8;
/*!40101 SET character_set_client = @saved_cs_client */;

--
-- Dumping data for table `top`
--

LOCK TABLES `top` WRITE;
/*!40000 ALTER TABLE `top` DISABLE KEYS */;
INSERT INTO `top` VALUES (1,'The Event Log Imperfection Patterns initiative started as a result of work being done by Arthur ter Hofstede, Suriadi Suriadi, Moe Wynn and Robert Andrews in 2015 on process mining in the healthcare domain.<br><br>The aim of this initiative is to provide a conceptual basis for event log pre-processing in preparation for a process mining analysis. In particular, the research provides a systematic approach to identifying and rectifying quality issues that might exist in event logs.<br><br>On this web site you will find detailed descriptions of a set of imperfection patterns that occur frequently in event logs. For each pattern, its \'signature\' is described, i.e. the way the pattern manifests itself and the way the presence of the pattern in a log may be recognised. Each pattern is illustrated with a \'real-world\' example and suggested remedial action/s are provided.<br><br>We encourage interactions with interested parties about this research and its applications. For example, researchers or practitioners can submit information about patterns they have observed in preparing event logs. We would also appreciate any feedback in relation to the existing patterns described in these pages (e.g. observations of the patterns in process mining analyses).<br><br>');
/*!40000 ALTER TABLE `top` ENABLE KEYS */;
UNLOCK TABLES;
/*!40103 SET TIME_ZONE=@OLD_TIME_ZONE */;

/*!40101 SET SQL_MODE=@OLD_SQL_MODE */;
/*!40014 SET FOREIGN_KEY_CHECKS=@OLD_FOREIGN_KEY_CHECKS */;
/*!40014 SET UNIQUE_CHECKS=@OLD_UNIQUE_CHECKS */;
/*!40101 SET CHARACTER_SET_CLIENT=@OLD_CHARACTER_SET_CLIENT */;
/*!40101 SET CHARACTER_SET_RESULTS=@OLD_CHARACTER_SET_RESULTS */;
/*!40101 SET COLLATION_CONNECTION=@OLD_COLLATION_CONNECTION */;
/*!40111 SET SQL_NOTES=@OLD_SQL_NOTES */;

-- Dump completed on 2015-10-28 22:02:58
